# FDS cleanup and finalization
These are the procedures for 2023 data.

First we need to fill in everything in the Step 1 sheet. 

Project Title: Same as report - "Restoring Fish Passage in the Peace Region - 2023"
Company/Agency: Other
Company/Agency (Other): New Graph Environment Ltd.
Spreadsheet Recorder(s): Allan Irvine or Lucy Schick
Project Type: Research
PROVINCIAL PERMIT NUMBER: `permit_id` 


The information in this submission has been reviewed and verified by a Registered Professional Biologist (Pick list: Yes or No)					Yes	
Biologist's Name:					Allan Irvine	
Registration Number:					2775	
Province of Registration:					British Columbia	

## Define paths and params
Some of this could move to params in the future but for now we will just define them here.

```{r}
#location of the data
repo_name <- "fish_passage_peace_2023_reporting"
permit_id <- 'PG23-813101'

stub_repo <- fs::path("~/Projects/repo", repo_name)

path_in <- fs::path(stub_repo, "data/habitat_confirmations.xls")

stub_out_csv <-  fs::path(stub_repo, "data/inputs_extracted")

stub_out_permit <- fs::path(stub_repo, "data/permit_submission")

fs::dir_create(stub_out_permit)

```


## Extract `stream_crossing_id`s
For now we get the stream crossing ids from the habitat confirmations sheet.  We will use these to query the database for the watershed codes.
In the future we could perhaps use `form_fiss_site`.
```{r extract-ids}
# define a list of stream_crossing_id 
hab_con <- fpr::fpr_import_hab_con(path = path_in,
                                               backup = FALSE,
                                               row_empty_remove = T,
                                               col_filter_na = TRUE) 

ids <- hab_con |> 
  purrr::pluck("step_1_ref_and_loc_info") |> 
  tidyr::separate(alias_local_name, into = c('site', 'location', 'ef'), remove = F) |>
  dplyr::distinct(site) |> 
  dplyr::pull(site)  

```

## Query database to get 1:50,000 watershed codes
Really kind of humorous that we are getting 1:50,000 watershed codes from the database then the province turns around
and converts them back to 1:20,000 (pers. comm. Dave McEwan - Fisheries Standards Biologist - 778 698-4010 - Dave.McEwan@gov.bc.ca).

```{r get-wsc}
conn <- fpr::fpr_db_conn()

wscodes <- fpr::fpr_db_query(
  query = glue::glue_sql("SELECT DISTINCT ON (stream_crossing_id)
    a.stream_crossing_id,
    a.linear_feature_id,
    a.watershed_group_code,
    b.watershed_code_50k,
    substring(b.watershed_code_50k from 1 for 3)
    || '-' || substring(b.watershed_code_50k from 4 for 6)
    || '-' || substring(b.watershed_code_50k from 10 for 5)
    || '-' || substring(b.watershed_code_50k from 15 for 5)
    || '-' || substring(b.watershed_code_50k from 20 for 4)
    || '-' || substring(b.watershed_code_50k from 24 for 4)
    || '-' || substring(b.watershed_code_50k from 28 for 3)
    || '-' || substring(b.watershed_code_50k from 31 for 3)
    || '-' || substring(b.watershed_code_50k from 34 for 3)
    || '-' || substring(b.watershed_code_50k from 37 for 3)
    || '-' || substring(b.watershed_code_50k from 40 for 3)
    || '-' || substring(b.watershed_code_50k from 43 for 3) AS watershed_code_50k_parsed,
    b.blue_line_key_20k,
    b.watershed_key_20k,
    b.blue_line_key_50k,
    b.watershed_key_50k,
    b.match_type
    FROM bcfishpass.crossings a
    LEFT OUTER JOIN whse_basemapping.fwa_streams_20k_50k b
    ON a.linear_feature_id = b.linear_feature_id_20k
    WHERE a.stream_crossing_id IN ({ids*})
    ORDER BY a.stream_crossing_id, b.match_type;",
    .con = conn
  ) 
)
DBI::dbDisconnect(conn)
```


<br>

Now make csv with watershed codes and and burn to file - so we can copy and paste into the spreadsheet. We need to
QA this data using the `whse_fish.wdic_waterbody_route_line_svw` layer.  This layer has now been added the
2023 QGIS shared projects for Skeena and Peace:

```{r csv-wsc}

##now we need to join to our habitat_confirmations sheet in the right order so we can copy and past into the spreadsheet
wsc_join <- dplyr::left_join(
  hab_con |> 
  purrr::pluck("step_1_ref_and_loc_info") |> 
    tidyr::separate(alias_local_name, into = c('site', 'location', 'ef'), remove = F) |> 
    dplyr::mutate(site = as.integer(site)),
  
  wscodes |> dplyr::select(stream_crossing_id, watershed_code_50k = watershed_code_50k_parsed, watershed_group_code),
  
  by = c('site' = 'stream_crossing_id')) |> 
  
  dplyr::select(reference_number, gazetted_name, alias_local_name, site, location, watershed_code_50k, watershed_group_code) |> 
  dplyr::mutate(alias_corrected = NA_character_,
         waterbody_id = paste0('00000', watershed_group_code),
         waterbody_type = 'stream') |> 
  # QA in QGIS using the wdic_waterbody_route_line_svw layer in postgres db, this layer comes from:
  # https://catalogue.data.gov.bc.ca/dataset/wsa-stream-routes-50-000/resource/d694884b-97a6-4d34-9ece-263aa46974f5
  # here are some hand bomb corrections
  # mutate(watershed_code_50k = case_when(
  #   site == 197974 ~ '460-829700-20600-00000-0000-0000-000-000-000-000-000-000',
  #   site == 198066 ~ '460-517700-00000-00000-0000-0000-000-000-000-000-000-000',
  #   T ~ watershed_code_50k)) %>%
  dplyr::select(reference_number, alias_local_name, site, location, gazetted_name, alias_corrected, waterbody_type, waterbody_id, watershed_code_50k)


# order to location spreadsheet and burn to file
wsc_join |> 
  readr::write_csv(file = fs::path(stub_out_csv, "hab_con_wsc_codes.csv"), na = "")
```


At this point we do our hand work to transfer over the watershed codes to the `habitat_confirmations.xls` file from the csv.

## Duplicate the FDS and make final changes for submission
Now we will make a copy of the data. We don't love having multiple values of the same data but we don't want to break any of
the past workflows.

```{r}
fs::file_copy(path = path_in,
          new_path = fs::path(stub_out_permit, paste0(permit_id, "_data.xls")),
          overwrite = TRUE)

```

<br>

Now we reorganize it so that we have one location per site in `step_1_ref_and_loc_info`
that is a minimum of 100m long.  So - all the info we have been collecting as separate electrofishing sites
needs to be combined into one site.  How we will do this by sheet in the workbook:

- `step_1_ref_and_loc_info`
  + filter out sites with `ef` in the location column generated from the `alias_local_name` column

- `step_2_fish_coll_data`
  + replace `method_number` for each site with number appended to `_ef` portion of the `alias_local_name` (ex. the `1` 
  becomes the method number extracted from `alias_local_name` = `125231_us_ef1` and `2` becomes the method number extracted from `125231_us_ef2`).
  + Append comments from `step_4_stream_site_data` to the comments column.
  + replace the existing `reference_number` for all `*_ef*` sites with the `reference_number` of the site that the `alias_local_name` is associated with.

- `step_3_individual_fish_data`
  + replace the existing `reference_number` for all `*_ef*` sites with the `reference_number` of the site that the `alias_local_name` is associated with.
  + replace `method_number` for each site with number appended to `_ef` portion of the `alias_local_name` as per `step_2_fish_coll_data`.
  
- `step_4_stream_site_data`
  + Append `alias_local_name` to the comments column.
  + remove all `*_ef*` sites. This will require erasing everything and then pasting back in the data from the dataframe.
  

### `step_4_stream_site_data` - Copy the `alias_local_name` to `comments` 

We now do all work on the `data/permit_submission/{permit_number}_data.xls` file.

<br>

Apparently it does not matter if we leave our unique id in this column but it is not going to land in provincial datasets
if we don't put it somewhere else. We want people to be able to cross reference data in the provincial systems to our reports so we add the id to the comments of `step_4_stream_site_data` sheet.  

```{r copy-alias-step4}

# now make a csv with the alias_local_name pasted to the comments so people can see which comments are linked to which site in the reports
site_comments <- hab_con |> 
  purrr::pluck("step_4_stream_site_data") |> 
  dplyr::mutate(comments = paste0('Site ', local_name, '. ', comments)) |> 
  dplyr::select(reference_number, gazetted_names, local_name, comments)

site_comments |> 
  readr::write_csv(fs::path(stub_out_csv, "hab_con_alias_local_name_relocate.csv"))
```

<br>

Now we do the hand work to copy-paste-special-values this data over to the `data/permit_submission/{permit_number}_data.xls` file.  

<br>

### `step_2_fish_coll_data` - Transfer comments and `UTMs` from `step_4_stream_site_data`, extract `method_number` and re-assign `reference_number`.
We also add all comments (now with site ids included) along with the site UTMs to the comments of the `step_2_fish_coll_data` sheet of the fds because IF WE HAVE A 100m+ HABITAT SITE - one of our next steps is to remove the `_ef*` sites from `step_4_stream_site_data` we want to link the electrofishing site ids and comments with the fish collection sites (only place where that info can now live....). It's going to result in redundant
comments and may not end up accessible in the data layers we pull from BC Data Catalogue but we do it in case it is helpful.


<br>

First we run a test to see if we need to update `reference numbers `and `method_number.`  If there is an `ef` in the `alias_local_name` column AND
there is an associated site that has the same "12345_us` but does not have the "_ef*" in the `alias_local_name`
then we will update the reference number for the ef site to the reference number of the non-ef site. If there is no
associated us or ds site then we will leave the reference number as is.  We need the `alias_local_name` column
from step 1 of the form.  First step is just to answer the question of which sites have a non-ef site associated with them.

```{r test-reference-update}

# these sites have no ef
site_match_prep <-   hab_con |> 
    purrr::pluck("step_1_ref_and_loc_info") |> 
    dplyr::select(reference_number, alias_local_name) |> 
    tidyr::separate(alias_local_name, into = c('site', 'location', 'ef'), remove = F) |> 
  #now make a column that has the site and location put back together as site_id
  dplyr::mutate(site_id = paste(site, location, sep = "_")) 


# these sites have no ef
site_match_na_ef <- site_match_prep |> 
  dplyr::filter(is.na(ef)) 

# these sites have ef
site_match_ef <- site_match_prep |> 
  dplyr::filter(!is.na(ef)) |> 
  dplyr::pull(site_id) 

# filter out the site_match_na_ef that have a corresponding ef site
site_match_na_ef_yes <- site_match_na_ef |> 
  dplyr::filter(site_id %in% site_match_ef) 

```

<br>

Ok - the script below works because we do not have any reference numbers to update. It does update the `method_number`
part but not the `reference_number` (didn't get that far before realizing we had no matches for Peace. If we actually do need to update the
`reference_number` (Zymoetz crossing from Skeena 2023) and the `method_number`  then we will need to script in the
part to update the `reference_number`. May be a pain but should be doable. Hard to know if hand-bombing will be much simpler. 
It may be so perhaps that is the route we wil go. This whole thing is only happening for two files so already we are probably
past the threshold of all this scripting being worth it. dang.


```{r copy-comments-step2}

#so we update the method number only when the site has an ef site associated with it
site_comments_prep <- site_comments |> 
    dplyr::select(reference_number, comments) |>
  #tidyr::replace_na with "" so we don't get NAs
  tidyr::replace_na(list(comments = ""))

step_2_prep <- hab_con |> 
      purrr::pluck("step_2_fish_coll_data") |> 
      dplyr::select(reference_number, local_name, method_number, comments_step2 = comments) |> 
      #tidyr::replace_na with "" so we don't get NAs
      tidyr::replace_na(list(comments_step2 = ""))

site_comments_step2_prep <- dplyr::left_join(
  
  if(nrow(site_match_na_ef_yes) > 0){
    step_2_prep |> 
      tidyr::separate(local_name, into = c('site', 'location', 'ef'), remove = F) |>
      # sub in the haul_number_pass_number for the method_number appended to the end of ef
      dplyr::mutate(ef = stringr::str_replace(ef, "ef", ""),
                    method_number = ef) 
  } else {
    step_2_prep},
    
    site_comments_prep,
    
    by = 'reference_number') |> 
  dplyr::mutate(comments_step2 = dplyr::case_when(!is.na(comments_step2) ~ paste(comments, comments_step2, sep = " "),
                                                  TRUE ~ comments)) |> 
  dplyr::select(reference_number, local_name, method_number, comments_step2)

#now we need the utms for the comments if we ARE ammending reference numbers


site_comments_step2 <- if(nrow(site_match_na_ef_yes) > 0){
  
  dplyr::left_join(
  
  site_comments_step2_prep,
  
  hab_con |> 
  purrr::pluck("step_1_ref_and_loc_info") |> 
  dplyr::select(reference_number, utm_zone, utm_easting, utm_northing),
  
  by = 'reference_number') |>
  dplyr::mutate(comments_step2 = paste(comments_step2, "UTM:", utm_zone, utm_easting, utm_northing, sep = " "))
} else {
  
  site_comments_step2_prep
}

# burn out our csv with the updated comments (and method number and utms if necessary)
site_comments_step2 |> 
    readr::write_csv(fs::path(stub_out_csv, "hab_con_method_comments_step2.csv"))

```

<br>

Ok - now we do the hand work to transfer `data/inputs_extracted/hab_con_method_comments_step2.csv` to the `data/permit_submission/{permit_number}_data.xls` file.


### Remove `step_4_stream_site_data` sites that are `_ef*` sites 
We have lots of sites that are return visits where we are just sampling and not doing 100m habitat sites.
For these we will leave all the locational information in the `step_1_ref_and_loc_info` sheet and remove all the `step_4_stream_site_data`.
This way we will at least not clutter the database with information that is not required or desired. We will leave the 
`step_3_individual_fish_data` sheet even though it is not actually required because it seems useful.

```{r remove-ef-sites}

# now we remove the ef sites from the step_4_stream_site_data sheet
step_4_prep <- hab_con |> 
  purrr::pluck("step_4_stream_site_data") |> 
  dplyr::filter(!grepl("ef", local_name))

# burn out our csv so we can hand-bomb
step_4_prep |> 
  readr::write_csv(fs::path(stub_out_csv, "hab_con_step4_no_ef.csv"))


```

<br>

Might be a better way but removed all sites and copy pasted back in the data from the csv.  Now we have a clean `step_4_stream_site_data` sheet.

